{"cells":[{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1630597918894,"user":{"displayName":"Byung-Woo Hong","photoUrl":"","userId":"17273657108187629509"},"user_tz":-540},"id":"K29I-OwCEYzW","outputId":"db8b297c-a4b8-457a-b43f-7570135306c7"},"source":["# Image Segmentation by Supervised Learning"]},{"cell_type":"markdown","metadata":{"id":"o5tna-SZuFX7"},"source":["## import libraries"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"WsBGxeOEuFX8","executionInfo":{"status":"ok","timestamp":1668753440629,"user_tz":-540,"elapsed":574,"user":{"displayName":"방윤하","userId":"13989411945591696000"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from tqdm import tqdm\n","import random\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab Notebooks/MachineLearning2022-2/MachineLearning2022-2"],"metadata":{"id":"esuNvr6quLqT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CTKVzKLiuFX_"},"source":["## load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMyKvtdUuFX_"},"outputs":[],"source":["directory_data  = './'\n","filename_data   = 'assignment_09_data.npz'\n","data            = np.load(os.path.join(directory_data, filename_data))\n","\n","image_train     = data['image_train']\n","mask_train      = data['mask_train']\n","\n","image_test      = data['image_test']\n","mask_test       = data['mask_test']\n","\n","num_data_train  = image_train.shape[0]\n","num_data_test   = image_test.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpvuBp4JuFYA"},"outputs":[],"source":["print('*************************************************')\n","print('size of x_train :', image_train.shape)\n","print('size of y_train :', mask_train.shape)\n","print('*************************************************')\n","print('size of x_test :', image_test.shape)\n","print('size of y_test :', mask_test.shape)\n","print('*************************************************')"]},{"cell_type":"markdown","metadata":{"id":"wheVUhnCuFYC"},"source":["## plot data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbiW7_KouFYD"},"outputs":[],"source":["def plot_image(title, image, mask):\n","    \n","    nRow = 2\n","    nCol = 4\n","    size = 3\n","    \n","    fig, axes = plt.subplots(nRow, nCol, figsize=(size * nCol, size * nRow))\n","    fig.suptitle(title, fontsize=16)\n","    \n","    for c in range(nCol):\n","        axes[0, c].imshow(image[c], cmap='gray')\n","        axes[1, c].imshow(mask[c], cmap='gray', vmin=0, vmax=1)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrTGOHuruFYE"},"outputs":[],"source":["plot_image('training data', image_train, mask_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HX4xQFtKuFYF"},"outputs":[],"source":["plot_image('testing data', image_test, mask_test)"]},{"cell_type":"markdown","metadata":{"id":"fUIuauBGuFYG"},"source":["## custom data loader for the PyTorch framework"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEUfkhMAuFYG"},"outputs":[],"source":["class dataset(Dataset):\n","    \n","    def __init__(self, image, mask, use_transform=False):\n","        \n","        self.image          = image\n","        self.mask           = mask\n","        self.use_transform  = use_transform\n","    \n","    def __getitem__(self, index):\n","        \n","        image   = self.image[index]\n","        mask    = self.mask[index]\n","\n","        image   = torch.FloatTensor(image).unsqueeze(dim=0)\n","        mask    = torch.FloatTensor(mask).unsqueeze(dim=0)\n","\n","        if self.use_transform:\n","            # ==================================================\n","            # add codes for applying data augmentation \n","            #\n","                #rotate 90 degrees\n","                image   = transforms.RandomRotation(degrees = 45)(image)\n","                mask    = transforms.RandomRotation(degrees = 45)(mask)\n","            \n","            #    \n","            # ==================================================\n","\n","        return (image, mask)\n","    \n","    def __len__(self):\n","\n","        number_image = self.image.shape[0]\n","\n","        return number_image"]},{"cell_type":"markdown","metadata":{"id":"tDRBNj7EuFYG"},"source":["## setting device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SnAGyvuuFYH"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'mps')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXK40jKauFYH"},"outputs":[],"source":["print(device)"]},{"cell_type":"markdown","metadata":{"id":"bvsOVyUFuFYI"},"source":["## construct datasets and dataloaders for testing and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Un2QqRRiuFYI"},"outputs":[],"source":["# ==================================================\n","# determine the mini-batch size\n","#\n","size_minibatch      = 10\n","#\n","# ==================================================\n","\n","dataset_train       = dataset(image_train, mask_train, True)\n","dataset_test        = dataset(image_test, mask_test, False)\n","\n","dataloader_train    = torch.utils.data.DataLoader(dataset_train, batch_size=size_minibatch, shuffle=True, drop_last=True)\n","dataloader_test     = torch.utils.data.DataLoader(dataset_test, batch_size=size_minibatch, shuffle=False, drop_last=False)"]},{"cell_type":"markdown","metadata":{"id":"8ngJeHXH2dfj"},"source":["## construct a neural network "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjlHwUOV-UCh"},"outputs":[],"source":["class Network(nn.Module):\n","    def __init__(self):\n","        super(Network,self).__init__()\n","\n","        # -------------------------------------------------\n","        # Encoder\n","        # -------------------------------------------------\n","        self.encoder_layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=1, bias=True),  \n","            nn.BatchNorm2d(8),\n","            nn.ReLU(),\n","        )\n","        \n","        self.encoder_layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1, bias=True),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","        )\n","\n","        # -------------------------------------------------\n","        # Decoder\n","        # -------------------------------------------------\n","        self.decoder_layer2 = nn.Sequential(\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.BatchNorm2d(8),\n","            nn.ReLU(),\n","        )\n","        \n","        self.decoder_layer1 = nn.Sequential(\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","            nn.Conv2d(in_channels=8, out_channels=1, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Sigmoid(),\n","        )\n","\n","        # -------------------------------------------------\n","        # Network\n","        # -------------------------------------------------\n","        self.network = nn.Sequential(\n","            self.encoder_layer1,\n","            self.encoder_layer2,\n","            self.decoder_layer2, \n","            self.decoder_layer1,\n","        )\n","\n","        self.initialize_weight()\n","\n","    def forward(self,x):\n","    \n","        out = self.network(x)\n","      \n","        return out\n","\n","    # ======================================================================\n","    # initialize weights\n","    # ======================================================================\n","    def initialize_weight(self):\n","            \n","        for m in self.network.modules():\n","            \n","            if isinstance(m, nn.Conv2d):\n","\n","                nn.init.xavier_uniform_(m.weight) \n","                if m.bias is not None:\n","\n","                    nn.init.constant_(m.bias, 1)\n","                    pass\n","                    \n","            elif isinstance(m, nn.BatchNorm2d):\n","                \n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 1)\n","                \n","            elif isinstance(m, nn.Linear):\n","\n","                nn.init.xavier_uniform_(m.weight)\n","\n","                if m.bias is not None:\n","                    \n","                    nn.init.constant_(m.bias, 1)\n","                    pass"]},{"cell_type":"markdown","metadata":{"id":"vTXZnsrGLoxg"},"source":["## build the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsGGOsXKLuc1"},"outputs":[],"source":["model       = Network().to(device)\n","\n","# ==================================================\n","# determine the optimiser and its associated hyper-parameters\n","#\n","learning_rate   = 0.00001\n","alpha           = 0.1\n","number_epoch    = 20\n","optimizer       = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","#\n","# =================================================="]},{"cell_type":"markdown","metadata":{"id":"6yE9LChkQ5G2"},"source":["## compute the prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mgS8kzGQ502"},"outputs":[],"source":["def compute_prediction(model, input):\n","\n","    prediction = model(input)\n","\n","    return prediction"]},{"cell_type":"markdown","metadata":{"id":"UQVz0ChdM9KL"},"source":["## compute the loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Y0K16g3uFYL"},"outputs":[],"source":["def compute_loss_data_fidelity(prediction, mask):\n","    # ==================================================\n","    # fill up the blank\n","    #\n","\n","    temp            = -(mask * torch.log(prediction)) - (1-mask) * torch.log(1-prediction)\n","    loss    = torch.mean(temp)\n","\n","    #\n","    # ==================================================\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hbp8AnSPuFYM"},"outputs":[],"source":["def compute_loss_regularization(prediction):\n","    # ==================================================\n","    # fill up the blank\n","    #\n","    \n","    bs, c, h, w = prediction.size()\n","    height = torch.abs(prediction[:, :, 1:, :] - prediction[:, :, :- 1, :]).sum()\n","    width  = torch.abs(prediction[:, :, :, 1:] - prediction[:, :, :, :-1]).sum()\n","\n","    loss = (height + width) / (bs * c * h * w)\n","   \n","    #\n","    # ==================================================\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbvdtlxKNAmm"},"outputs":[],"source":["def compute_loss(prediction, mask, alpha):\n","    # ==================================================\n","    # fill up the blank\n","    #\n","\n","    loss_data_fidelity  = compute_loss_data_fidelity(prediction, mask)\n","    loss_regularization = compute_loss_regularization(prediction)\n","    loss                = loss_data_fidelity + alpha * loss_regularization\n","    \n","    #\n","    # ==================================================\n","\n","    return (loss, loss_data_fidelity, loss_regularization)"]},{"cell_type":"markdown","metadata":{"id":"edyvSh7fuFYM"},"source":["## compute the loss value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M136SbsfuFYM"},"outputs":[],"source":["def compute_loss_value(loss):\n","    \n","    loss_value = loss.item()\n","    \n","    return loss_value"]},{"cell_type":"markdown","metadata":{"id":"ZfD48LjNuFYN"},"source":["## compute the accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGjvb-KyuFYN"},"outputs":[],"source":["def compute_accuracy(prediction, mask):\n","    \n","    prediction  = prediction.squeeze(axis=1)\n","    binary      = (prediction >= 0.5)\n","    mask        = mask.squeeze(axis=1).bool()\n","    \n","    intersection = (binary & mask).float().sum((1, 2))\n","    union        = (binary | mask).float().sum((1, 2))\n","\n","    eps         = 1e-8\n","    correct     = (intersection + eps) / (union + eps)\n","    accuracy    = correct.mean() * 100.0\n","    accuracy    = accuracy.cpu() \n","    \n","    return accuracy"]},{"cell_type":"markdown","metadata":{"id":"9UIl4gCp2hE6"},"source":["## Variable for the learning curves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXs-w_99-cuZ"},"outputs":[],"source":["loss_train_mean     = np.zeros(number_epoch)\n","loss_train_std      = np.zeros(number_epoch)\n","accuracy_train_mean = np.zeros(number_epoch)\n","accuracy_train_std  = np.zeros(number_epoch)\n","\n","loss_test_mean      = np.zeros(number_epoch)\n","loss_test_std       = np.zeros(number_epoch)\n","accuracy_test_mean  = np.zeros(number_epoch)\n","accuracy_test_std   = np.zeros(number_epoch)\n","\n","loss_train_data_fidelity_mean   = np.zeros(number_epoch)\n","loss_train_data_fidelity_std    = np.zeros(number_epoch)\n","loss_train_regularization_mean  = np.zeros(number_epoch)\n","loss_train_regularization_std   = np.zeros(number_epoch)\n","\n","loss_test_data_fidelity_mean    = np.zeros(number_epoch)\n","loss_test_data_fidelity_std     = np.zeros(number_epoch)\n","loss_test_regularization_mean   = np.zeros(number_epoch)\n","loss_test_regularization_std    = np.zeros(number_epoch)"]},{"cell_type":"markdown","metadata":{"id":"_1soRjtpuFYN"},"source":["## train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Jlts-WouFYO"},"outputs":[],"source":["def train(model, optimizer, dataloader):\n","\n","    loss_epoch                  = []\n","    loss_data_fidelity_epoch    = []\n","    loss_regularization_epoch   = []\n","    accuracy_epoch              = []\n","\n","    model.train()\n","\n","    for index_batch, (image, mask) in enumerate(dataloader):\n","\n","        image       = image.to(device)\n","        mask        = mask.to(device)\n","        \n","        # ==================================================\n","        # fill up the blank\n","        #\n","        prediction  = compute_prediction(model, image)\n","        (loss, loss_data_fidelity, loss_regularization) = compute_loss(prediction, mask, alpha)\n","        \n","        loss_value                  = compute_loss_value(loss)\n","        loss_data_fidelity_value    = compute_loss_value(loss_data_fidelity)\n","        loss_regularization_value   = compute_loss_value(loss_regularization)\n","        accuracy                    = compute_accuracy(prediction, mask)\n","        #\n","        # ==================================================\n","\n","        loss_epoch.append(loss_value)\n","        loss_data_fidelity_epoch.append(loss_data_fidelity_value)\n","        loss_regularization_epoch.append(loss_regularization_value)\n","        accuracy_epoch.append(accuracy)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    loss_mean   = np.mean(loss_epoch)\n","    loss_std    = np.std(loss_epoch)\n","\n","    loss_data_fidelity_mean = np.mean(loss_data_fidelity_epoch)\n","    loss_data_fidelity_std  = np.std(loss_data_fidelity_epoch)\n","\n","    loss_regularization_mean    = np.mean(loss_regularization_epoch)\n","    loss_regularization_std     = np.std(loss_regularization_epoch)\n","    \n","    accuracy_mean   = np.mean(accuracy_epoch)\n","    accuracy_std    = np.std(accuracy_epoch)\n","\n","    loss                = {'mean' : loss_mean, 'std' : loss_std}\n","    loss_data_fidelity  = {'mean' : loss_data_fidelity_mean, 'std' : loss_data_fidelity_std}\n","    loss_regularization = {'mean' : loss_regularization_mean, 'std' : loss_regularization_std}\n","    accuracy            = {'mean' : accuracy_mean, 'std' : accuracy_std}\n","\n","    return (loss, loss_data_fidelity, loss_regularization, accuracy)"]},{"cell_type":"markdown","metadata":{"id":"-rUZADmxuFYO"},"source":["## test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xb5SWCAhuFYO"},"outputs":[],"source":["def test(model, dataloader):\n","\n","    loss_epoch                  = []\n","    loss_data_fidelity_epoch    = []\n","    loss_regularization_epoch   = []\n","    accuracy_epoch              = []\n","\n","    model.eval()\n","\n","    for index_batch, (image, mask) in enumerate(dataloader):\n","\n","        image       = image.to(device)\n","        mask        = mask.to(device)\n","\n","        # ==================================================\n","        # fill up the blank\n","        #\n","        prediction  = compute_prediction(model, image)\n","        (loss, loss_data_fidelity, loss_regularization) = compute_loss(prediction, mask, alpha)\n","        \n","        loss_value                  = compute_loss_value(loss)\n","        loss_data_fidelity_value    = compute_loss_value(loss_data_fidelity)\n","        loss_regularization_value   = compute_loss_value(loss_regularization)\n","        accuracy                    = compute_accuracy(prediction, mask)\n","        #\n","        # ==================================================\n","        \n","        loss_epoch.append(loss_value)\n","        loss_data_fidelity_epoch.append(loss_data_fidelity_value)\n","        loss_regularization_epoch.append(loss_regularization_value)\n","        accuracy_epoch.append(accuracy)\n","\n","    loss_mean   = np.mean(loss_epoch)\n","    loss_std    = np.std(loss_epoch)\n","\n","    loss_data_fidelity_mean = np.mean(loss_data_fidelity_epoch)\n","    loss_data_fidelity_std  = np.std(loss_data_fidelity_epoch)\n","\n","    loss_regularization_mean    = np.mean(loss_regularization_epoch)\n","    loss_regularization_std     = np.std(loss_regularization_epoch)\n","    \n","    accuracy_mean   = np.mean(accuracy_epoch)\n","    accuracy_std    = np.std(accuracy_epoch)\n","\n","    loss                = {'mean' : loss_mean, 'std' : loss_std}\n","    loss_data_fidelity  = {'mean' : loss_data_fidelity_mean, 'std' : loss_data_fidelity_std}\n","    loss_regularization = {'mean' : loss_regularization_mean, 'std' : loss_regularization_std}\n","    accuracy            = {'mean' : accuracy_mean, 'std' : accuracy_std}\n","\n","    return (loss, loss_data_fidelity, loss_regularization, accuracy)"]},{"cell_type":"markdown","metadata":{"id":"XuGBUozBuFYP"},"source":["## train and test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6Qtqgv8-fMW"},"outputs":[],"source":["# ================================================================================\n","# \n","# iterations for epochs\n","#\n","# ================================================================================\n","for i in tqdm(range(number_epoch)):\n","    \n","    # ================================================================================\n","    # \n","    # training\n","    #\n","    # ================================================================================\n","    (loss_train, loss_data_fidelity_train, loss_regularization_train, accuracy_train) = train(model, optimizer, dataloader_train)\n","\n","    loss_train_mean[i]  = loss_train['mean']\n","    loss_train_std[i]   = loss_train['std']\n","\n","    loss_train_data_fidelity_mean[i]    = loss_data_fidelity_train['mean']\n","    loss_train_data_fidelity_std[i]     = loss_data_fidelity_train['std']\n","\n","    loss_train_regularization_mean[i]   = loss_regularization_train['mean']\n","    loss_train_regularization_std[i]    = loss_regularization_train['std']\n","\n","    accuracy_train_mean[i]  = accuracy_train['mean']\n","    accuracy_train_std[i]   = accuracy_train['std']\n","\n","    # ================================================================================\n","    # \n","    # testing\n","    #\n","    # ================================================================================\n","    (loss_test, loss_data_fidelity_test, loss_regularization_test, accuracy_test) = test(model, dataloader_test)\n","\n","    loss_test_mean[i]  = loss_test['mean']\n","    loss_test_std[i]   = loss_test['std']\n","\n","    loss_test_data_fidelity_mean[i]    = loss_data_fidelity_test['mean']\n","    loss_test_data_fidelity_std[i]     = loss_data_fidelity_test['std']\n","\n","    loss_test_regularization_mean[i]   = loss_regularization_test['mean']\n","    loss_test_regularization_std[i]    = loss_regularization_test['std']\n","\n","    accuracy_test_mean[i]  = accuracy_test['mean']\n","    accuracy_test_std[i]   = accuracy_test['std']"]},{"cell_type":"markdown","metadata":{"id":"jaq6TcdPuFYP"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"58Sj1H8MuFYP"},"source":["## functions for presenting the results"]},{"cell_type":"markdown","metadata":{"id":"k9hk90CSuFYQ"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tp50r3lh-o-P"},"outputs":[],"source":["def function_result_01():\n","    \n","    title           = 'loss (training)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'loss'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(loss_train_mean)), loss_train_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(loss_train_mean)), loss_train_mean - loss_train_std, loss_train_mean + loss_train_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoVKe2ANuFYQ"},"outputs":[],"source":["def function_result_02():\n","    \n","    title           = 'loss - data fidelity (training)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'loss'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(loss_train_data_fidelity_mean)), loss_train_data_fidelity_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(loss_train_data_fidelity_mean)), loss_train_data_fidelity_mean - loss_train_data_fidelity_std, loss_train_data_fidelity_mean + loss_train_data_fidelity_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQAOAhAauFYQ"},"outputs":[],"source":["def function_result_03():\n","    \n","    title           = 'loss - regularization (training)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'loss'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(loss_train_regularization_mean)), loss_train_regularization_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(loss_train_regularization_mean)), loss_train_regularization_mean - loss_train_regularization_std, loss_train_regularization_mean + loss_train_regularization_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrxFXkkauFYQ"},"outputs":[],"source":["def function_result_04():\n","    \n","    title           = 'loss (testing)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'loss'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(loss_test_mean)), loss_test_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(loss_test_mean)), loss_test_mean - loss_test_std, loss_test_mean + loss_test_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jut-iJ5vuFYQ"},"outputs":[],"source":["def function_result_05():\n","    \n","    title           = 'loss - data fidelity (testing)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'loss'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(loss_test_data_fidelity_mean)), loss_test_data_fidelity_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(loss_test_data_fidelity_mean)), loss_test_data_fidelity_mean - loss_test_data_fidelity_std, loss_test_data_fidelity_mean + loss_test_data_fidelity_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTMQcc-puFYR"},"outputs":[],"source":["def function_result_06():\n","    \n","    title           = 'loss - regularization (testing)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'loss'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(loss_test_regularization_mean)), loss_test_regularization_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(loss_test_regularization_mean)), loss_test_regularization_mean - loss_test_regularization_std, loss_test_regularization_mean + loss_test_regularization_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5xZBuW0uFYR"},"outputs":[],"source":["def function_result_07():\n","    \n","    title           = 'accuracy (training)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'accuracy'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(accuracy_train_mean)), accuracy_train_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(accuracy_train_mean)), accuracy_train_mean - accuracy_train_std, accuracy_train_mean + accuracy_train_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DdBhqSyuFYS"},"outputs":[],"source":["def function_result_08():\n","    \n","    title           = 'accuracy (testing)'\n","    label_axis_x    = 'epoch' \n","    label_axis_y    = 'accuracy'\n","    color_mean      = 'red'\n","    color_std       = 'blue'\n","    alpha           = 0.3\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.title(title)\n","\n","    plt.plot(range(len(accuracy_test_mean)), accuracy_test_mean, '-', color = color_mean)\n","    plt.fill_between(range(len(accuracy_test_mean)), accuracy_test_mean - accuracy_test_std, accuracy_test_mean + accuracy_test_std, facecolor = color_std, alpha = alpha) \n","    \n","    plt.xlabel(label_axis_x)\n","    plt.ylabel(label_axis_y)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"av8cv5RnuFYS"},"outputs":[],"source":["def function_result_09():\n","\n","    nRow = 10\n","    nCol = 4 \n","    size = 3 \n","    \n","    title = 'training results'\n","    fig, axes = plt.subplots(nRow, nCol, figsize=(size * nCol, size * nRow))\n","    fig.suptitle(title, fontsize=16)\n","\n","    number_data = len(dataset_train)\n","    index_image = np.linspace(0, number_data-1, nRow).astype(int)\n","\n","    image       = torch.FloatTensor(dataset_train.image[index_image]).unsqueeze(dim=1).to(device)\n","    mask        = torch.FloatTensor(dataset_train.mask[index_image]).unsqueeze(dim=1).to(device)\n","    prediction  = compute_prediction(model, image)\n","\n","    image       = image.detach().cpu().squeeze(axis=1)\n","    mask        = mask.detach().cpu().squeeze(axis=1)\n","    prediction  = prediction.detach().cpu().squeeze(axis=1)\n","    binary      = (prediction >= 0.5)\n","\n","    for r in range(nRow):\n","\n","            axes[r, 0].imshow(image[r], cmap='gray')\n","            axes[r, 1].imshow(prediction[r], cmap='gray', vmin=0, vmax=1)\n","            axes[r, 2].imshow(binary[r], cmap='gray', vmin=0, vmax=1)\n","            axes[r, 3].imshow(mask[r], cmap='gray', vmin=0, vmax=1)\n","\n","            axes[r, 0].xaxis.set_visible(False)\n","            axes[r, 1].xaxis.set_visible(False)\n","            axes[r, 2].xaxis.set_visible(False)\n","            axes[r, 3].xaxis.set_visible(False)\n","            \n","            axes[r, 0].yaxis.set_visible(False)\n","            axes[r, 1].yaxis.set_visible(False)\n","            axes[r, 2].yaxis.set_visible(False)\n","            axes[r, 3].yaxis.set_visible(False)\n","            \n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZGvRFMYuFYS"},"outputs":[],"source":["def function_result_10():\n","    \n","    nRow = 10\n","    nCol = 4 \n","    size = 3 \n","\n","    title = 'testing results'\n","    fig, axes = plt.subplots(nRow, nCol, figsize=(size * nCol, size * nRow))\n","    fig.suptitle(title, fontsize=16)\n","\n","    number_data = len(dataset_test)\n","    index_image = np.linspace(0, number_data-1, nRow).astype(int)\n","    \n","    image       = torch.FloatTensor(dataset_test.image[index_image]).unsqueeze(dim=1).to(device)\n","    mask        = torch.FloatTensor(dataset_test.mask[index_image]).unsqueeze(dim=1).to(device)\n","    prediction  = compute_prediction(model, image)\n","\n","    image       = image.detach().cpu().squeeze(axis=1)\n","    mask        = mask.detach().cpu().squeeze(axis=1)\n","    prediction  = prediction.detach().cpu().squeeze(axis=1)\n","    binary      = (prediction >= 0.5)\n","\n","    for r in range(nRow):\n","    \n","            axes[r, 0].imshow(image[r], cmap='gray')\n","            axes[r, 1].imshow(prediction[r], cmap='gray', vmin=0, vmax=1)\n","            axes[r, 2].imshow(binary[r], cmap='gray', vmin=0, vmax=1)\n","            axes[r, 3].imshow(mask[r], cmap='gray', vmin=0, vmax=1)\n","\n","            axes[r, 0].xaxis.set_visible(False)\n","            axes[r, 1].xaxis.set_visible(False)\n","            axes[r, 2].xaxis.set_visible(False)\n","            axes[r, 3].xaxis.set_visible(False)\n","            \n","            axes[r, 0].yaxis.set_visible(False)\n","            axes[r, 1].yaxis.set_visible(False)\n","            axes[r, 2].yaxis.set_visible(False)\n","            axes[r, 3].yaxis.set_visible(False)\n","            \n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwoEenYbuFYT"},"outputs":[],"source":["def function_result_11():\n","    \n","    print('final training accuracy = %9.8f' % (accuracy_train_mean[-1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bn-ommcauFYT"},"outputs":[],"source":["def function_result_12():\n","    \n","    print('final testing accuracy = %9.8f' % (accuracy_test_mean[-1]))"]},{"cell_type":"markdown","metadata":{"id":"2ZwYgPxWuFYT"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"pnsY6z1MuFYT"},"source":["## results "]},{"cell_type":"markdown","metadata":{"id":"dw9B5gcFuFYT"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZATiUTfZuFYT"},"outputs":[],"source":["number_result = 12 \n","\n","for i in range(number_result):\n","\n","    title           = '# RESULT # {:02d}'.format(i+1) \n","    name_function   = 'function_result_{:02d}()'.format(i+1)\n","\n","    print('') \n","    print('################################################################################')\n","    print('#') \n","    print(title)\n","    print('#') \n","    print('################################################################################')\n","    print('') \n","\n","    eval(name_function)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCbe7QqjuFYU"},"outputs":[],"source":["!apt-get update"]},{"cell_type":"code","source":["!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc"],"metadata":{"id":"w2eo9FBpuWdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!jupyter nbconvert --to PDF '/content/drive/MyDrive/Colab Notebooks/MachineLearning2022-2/MachineLearning2022-2/assignment_09.ipynb'"],"metadata":{"id":"yCk8qbVhuaLx"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"}},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}